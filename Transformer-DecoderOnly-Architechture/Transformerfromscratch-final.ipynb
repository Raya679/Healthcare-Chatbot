{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.675055Z","iopub.execute_input":"2023-10-26T18:22:34.675602Z","iopub.status.idle":"2023-10-26T18:22:34.681704Z","shell.execute_reply.started":"2023-10-26T18:22:34.675538Z","shell.execute_reply":"2023-10-26T18:22:34.680430Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"2.0.0+cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self, vocab_size, embed_dim): #vocab_size: size of vocabulary, embed_dim: dimension of embeddings\n        super(Embedding, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input vector\n        Returns:\n            out: embedding vector\n        \"\"\"\n        out = self.embed(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.684107Z","iopub.execute_input":"2023-10-26T18:22:34.684470Z","iopub.status.idle":"2023-10-26T18:22:34.694987Z","shell.execute_reply.started":"2023-10-26T18:22:34.684438Z","shell.execute_reply":"2023-10-26T18:22:34.693866Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self,max_seq_len,embed_model_dim):#seq_len: length of input sequence,embed_model_dim: demension of embedding\n        super(PositionalEmbedding, self).__init__()\n        self.embed_dim = embed_model_dim\n\n        pe = torch.zeros(max_seq_len,self.embed_dim)\n        for pos in range(max_seq_len):\n            for i in range(0,self.embed_dim,2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input vector\n        Returns:\n            x: output\n        \"\"\"\n\n        # make embeddings relatively larger\n        x = x * math.sqrt(self.embed_dim)\n        #add constant to embedding\n        seq_len = x.size(1)\n        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.696293Z","iopub.execute_input":"2023-10-26T18:22:34.697449Z","iopub.status.idle":"2023-10-26T18:22:34.712492Z","shell.execute_reply.started":"2023-10-26T18:22:34.697413Z","shell.execute_reply":"2023-10-26T18:22:34.711407Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nimport math\n\ndef scaled_dot_product(Q,K,V):\n    d_k=Q.size()[-1]\n    scaled=torch.matmul(Q,K.transpose(-1,-2))/math.sqrt(d_k)\n    \n    mask=torch.full(scaled.size(),float('-inf'))\n    mask=torch.triu(mask,diagonal=1)\n    attention = func.softmax(scaled+mask,dim=-1)\n#     print(attention.size())\n#     print(V.size())\n    output=torch.matmul(attention,V)\n    print(output.size())\n    return output,attention\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self,d_model,num_heads):\n        super().__init__()\n        \n        self.d_model=d_model\n        self.num_heads=num_heads\n        self.head_dim=d_model//num_heads\n        self.qkv_layer=nn.Linear(d_model,3*d_model)\n        self.linear_layer=nn.Linear(d_model,d_model)  # (num_heads*d_v x d_model)\n        \n    def forward(self,x):\n        batch_size,n,d_model=x.size()\n        print(f\"x.size(): {x.size()}\")  \n        qkv=self.qkv_layer(x)  # (n x 3*d_model)\n        print(f\"qkv.size(): {qkv.size()}\")\n        qkv=self.qkv_layer(x)  # (n x 3*d_model)\n        print(f\"qkv.size(): {qkv.size()}\")\n        qkv=qkv.reshape(batch_size,self.num_heads,n,3*self.head_dim) # (n x d_model/num_heads) i.e per head\n        print(f\"qkv.size(): {qkv.size()} per head\")\n        \n        \n        q,k,v=qkv.chunk(3,dim=-1)\n        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}\")  # (n x d_model/(3*num_heads))\n              \n        output,attn=scaled_dot_product(q,k,v)\n        print(f\"output.size(): {output.size()}, attention.size:{ attn.size()}\")\n#         attn : (n x n), output : (n x d_v)\n        \n        output = output.reshape(batch_size, n, self.num_heads * self.head_dim)\n        print(f\"output.size(): {output.size()}\") # output : (n x num_heads*d_v) ..concatenating all heads\n        \n        out = self.linear_layer(output)\n        print(f\"out.size(): {out.size()}\")  # (n x d_model)\n              \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.714857Z","iopub.execute_input":"2023-10-26T18:22:34.715877Z","iopub.status.idle":"2023-10-26T18:22:34.732469Z","shell.execute_reply.started":"2023-10-26T18:22:34.715810Z","shell.execute_reply":"2023-10-26T18:22:34.731403Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# d_model = 512\n# num_heads = 8\n\n# batch_size = 30\n# sequence_length = 5\n# x = torch.randn((batch_size, sequence_length, d_model_dim))\n\n# model = MultiHeadAttention(d_model, num_heads)\n# out = model.forward(x)","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.734019Z","iopub.execute_input":"2023-10-26T18:22:34.734356Z","iopub.status.idle":"2023-10-26T18:22:34.749138Z","shell.execute_reply.started":"2023-10-26T18:22:34.734328Z","shell.execute_reply":"2023-10-26T18:22:34.748292Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"class LayerNorm1d: \n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\ndef parameters(self):\n    return [self.gamma, self.beta]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.750740Z","iopub.execute_input":"2023-10-26T18:22:34.751463Z","iopub.status.idle":"2023-10-26T18:22:34.768385Z","shell.execute_reply.started":"2023-10-26T18:22:34.751418Z","shell.execute_reply":"2023-10-26T18:22:34.767256Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    # a simple linear layer followed by a non-linearity\n\n    def __init__(self, n_embd, hidden):\n        super(FeedForward, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * hidden),  # Updated to use 'hidden'\n            nn.ReLU(),\n            nn.Linear(4 * hidden, n_embd),  # Updated to use 'hidden'\n            nn.Dropout(p=0.2),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.770039Z","iopub.execute_input":"2023-10-26T18:22:34.770709Z","iopub.status.idle":"2023-10-26T18:22:34.781952Z","shell.execute_reply.started":"2023-10-26T18:22:34.770676Z","shell.execute_reply":"2023-10-26T18:22:34.780905Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        \n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNorm1d(dim=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.ffn = FeedForward(n_embd = d_model,hidden=ffn_hidden)\n        self.norm2 = LayerNorm1d(dim=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y):\n        _y = y \n        print(\"MASKED MULTIHEAD ATTENTION\")\n        y = self.self_attention(y) \n        print(\"DROP OUT\")\n        y = self.dropout1(y) \n        print(\"ADD + NORM\")\n        y =  self.norm1(y + _y) \n\n        \n        _y = y  \n        print(\"FEED FORWARD\")\n        y = self.ffn(y) \n        print(\"DROP OUT\")\n        y = self.dropout2(y) \n        print(\"ADD + NORM\")\n        y = self.norm2(y + _y) \n        return y \n\nclass SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y = inputs\n        for module in self._modules.values():\n            y = module(x, y)\n        return y\nclass Decoder(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n        super().__init__()\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n                                          for _ in range(num_layers)])\n\n    def forward(self, x, y):\n        y = self.layers(x, y)\n        return y \n","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.832945Z","iopub.execute_input":"2023-10-26T18:22:34.833691Z","iopub.status.idle":"2023-10-26T18:22:34.846647Z","shell.execute_reply.started":"2023-10-26T18:22:34.833657Z","shell.execute_reply":"2023-10-26T18:22:34.845488Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"d_model = 512\nnum_heads = 8\ndrop_prob = 0.1\nbatch_size = 30\nmax_sequence_length = 200\nffn_hidden = 2048\nnum_layers = 5\n\nx = torch.randn( (batch_size, max_sequence_length, d_model) )  \ny = torch.randn( (batch_size, max_sequence_length, d_model) ) \n\ndecoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\nout = decoder(x, y)\n\nout.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-26T18:22:34.848707Z","iopub.execute_input":"2023-10-26T18:22:34.849463Z","iopub.status.idle":"2023-10-26T18:22:41.683414Z","shell.execute_reply.started":"2023-10-26T18:22:34.849415Z","shell.execute_reply":"2023-10-26T18:22:41.682303Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"MASKED MULTIHEAD ATTENTION\nx.size(): torch.Size([30, 200, 512])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 8, 200, 192]) per head\nq size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64])\ntorch.Size([30, 8, 200, 64])\noutput.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200])\noutput.size(): torch.Size([30, 200, 512])\nout.size(): torch.Size([30, 200, 512])\nDROP OUT\nADD + NORM\nFEED FORWARD\nDROP OUT\nADD + NORM\nMASKED MULTIHEAD ATTENTION\nx.size(): torch.Size([30, 200, 512])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 8, 200, 192]) per head\nq size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64])\ntorch.Size([30, 8, 200, 64])\noutput.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200])\noutput.size(): torch.Size([30, 200, 512])\nout.size(): torch.Size([30, 200, 512])\nDROP OUT\nADD + NORM\nFEED FORWARD\nDROP OUT\nADD + NORM\nMASKED MULTIHEAD ATTENTION\nx.size(): torch.Size([30, 200, 512])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 8, 200, 192]) per head\nq size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64])\ntorch.Size([30, 8, 200, 64])\noutput.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200])\noutput.size(): torch.Size([30, 200, 512])\nout.size(): torch.Size([30, 200, 512])\nDROP OUT\nADD + NORM\nFEED FORWARD\nDROP OUT\nADD + NORM\nMASKED MULTIHEAD ATTENTION\nx.size(): torch.Size([30, 200, 512])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 8, 200, 192]) per head\nq size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64])\ntorch.Size([30, 8, 200, 64])\noutput.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200])\noutput.size(): torch.Size([30, 200, 512])\nout.size(): torch.Size([30, 200, 512])\nDROP OUT\nADD + NORM\nFEED FORWARD\nDROP OUT\nADD + NORM\nMASKED MULTIHEAD ATTENTION\nx.size(): torch.Size([30, 200, 512])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 200, 1536])\nqkv.size(): torch.Size([30, 8, 200, 192]) per head\nq size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64])\ntorch.Size([30, 8, 200, 64])\noutput.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200])\noutput.size(): torch.Size([30, 200, 512])\nout.size(): torch.Size([30, 200, 512])\nDROP OUT\nADD + NORM\nFEED FORWARD\nDROP OUT\nADD + NORM\n","output_type":"stream"},{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"torch.Size([30, 200, 512])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}